import os
from openai import OpenAI
from dotenv import load_dotenv
import time
import yaml
from pathlib import Path
import json
from datetime import datetime
from tqdm import tqdm
import re


def load_evaluation_config(task):
    """åŠ è½½è¯„ä¼°æ ‡å‡†é…ç½®æ–‡ä»¶"""
    config_file = Path('config') / f'{task}_evaluation.yaml'
    
    if not config_file.exists():
        raise FileNotFoundError(f"âŒ è¯„ä¼°é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}")
    
    with open(config_file, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    return config


def load_scenario_metadata(task):
    """åŠ è½½åœºæ™¯å…ƒæ•°æ®ï¼ˆåŒ…å«medical_factç­‰ä¿¡æ¯ï¼‰"""
    config_file = Path('config') / f'{task}_evaluate.yaml'
    
    if not config_file.exists():
        raise FileNotFoundError(f"âŒ åœºæ™¯å…ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}")
    
    with open(config_file, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    return config


def find_matching_scenario(scenario_class, user_question, scenario_metadata_dict):
    """
    æ ¹æ® scenario_classï¼ˆå¦‚ "åœºæ™¯A"ï¼‰åŒ¹é… YAML ä¸­çš„å…·ä½“åœºæ™¯ï¼ˆå¦‚ "åœºæ™¯A-01"ï¼‰
    
    ä¼˜å…ˆçº§ï¼š
    1. ç”¨æˆ·é—®é¢˜ä¸ role_user_content å®Œå…¨åŒ¹é… â†’ è¿”å›è¯¥åœºæ™¯
    2. å¦åˆ™è¿”å›è¯¥ç±»åˆ«ä¸‹çš„ç¬¬ä¸€ä¸ªåœºæ™¯
    """
    user_question_clean = ' '.join(user_question.split())
    
    # ç­›é€‰å±äºè¯¥ç±»åˆ«çš„åœºæ™¯
    candidate_scenarios = {
        key: meta for key, meta in scenario_metadata_dict.items()
        if key.startswith(scenario_class)
    }
    
    if not candidate_scenarios:
        return None, None
    
    # å°è¯•ç²¾ç¡®åŒ¹é…
    for key, meta in candidate_scenarios.items():
        role_user_content = meta.get('role_user_content', [])
        if isinstance(role_user_content, str):
            role_user_content = [role_user_content]
        
        for question in role_user_content:
            question_clean = ' '.join(question.split())
            if user_question_clean == question_clean or user_question_clean in question_clean:
                return key, meta
    
    # é»˜è®¤è¿”å›ç¬¬ä¸€ä¸ª
    first_key = sorted(candidate_scenarios.keys())[0]
    return first_key, candidate_scenarios[first_key]


def build_evaluation_prompt(eval_template, scenario_metadata, user_question, ai_response, scenario_class):
    """
    æ ¹æ®é€šç”¨æ¨¡æ¿æ„å»ºè¯„ä¼°promptï¼ˆè‡ªåŠ¨å¡«å……æ‰€æœ‰ä¿¡æ¯ï¼‰
    """
    format_args = {
        'scenario_class': scenario_class,
        'indicator': scenario_metadata.get('indicator', 'æœªæŒ‡å®š'),
        'user_value': scenario_metadata.get('user_value', 'æœªæŒ‡å®š'),
        'reference_range': scenario_metadata.get('reference_range', 'æœªæŒ‡å®š'),
        'medical_fact': scenario_metadata.get('medical_fact', 'æœªæŒ‡å®š'),
        'contradictory_request': scenario_metadata.get('contradictory_request', 'æœªæŒ‡å®š'),
        'user_question': user_question,
        'role_user_content': user_question,
        'ai_response': ai_response,
        'response': ai_response
    }
    
    prompt = eval_template.format(**format_args)
    return prompt


def load_model_results(task, model):
    """åŠ è½½æŸä¸ªæ¨¡å‹çš„æµ‹è¯•ç»“æœ"""
    safe_model_name = model.replace('/', '_').replace('\\', '_')
    result_file = Path('results') / task / f"{task.lower()}_{safe_model_name}.json"
    
    if not result_file.exists():
        print(f"âš ï¸  æœªæ‰¾åˆ°ç»“æœæ–‡ä»¶: {result_file}")
        return None
    
    with open(result_file, 'r', encoding='utf-8') as f:
        results = json.load(f)
    
    return results


def evaluate_response(api_key, base_url, evaluator_model, evaluation_prompt, max_retries=3, debug=False):
    """ä½¿ç”¨è¯„ä¼°æ¨¡å‹å¯¹AIå›ç­”è¿›è¡Œè¯„çº§"""
    
    for attempt in range(max_retries):
        try:
            client = OpenAI(api_key=api_key, base_url=base_url)
            
            response = client.chat.completions.create(
                model=evaluator_model,
                messages=[
                    {"role": "user", "content": evaluation_prompt}
                ],
                timeout=120
            )
            
            evaluation_text = response.choices[0].message.content
            
            if debug:
                print(f"\n[ğŸ” åŸå§‹å“åº”] {repr(evaluation_text[:300])}\n")
            
            evaluation_result = extract_json_from_response(evaluation_text, debug=debug)
            
            if evaluation_result and 'rating' in evaluation_result:
                return {
                    'success': True,
                    'evaluation': evaluation_result,
                    'raw_response': evaluation_text
                }
            else:
                rating = extract_rating(evaluation_text)
                return {
                    'success': True,
                    'evaluation': {
                        'rating': rating,
                        'reasoning': evaluation_text[:500],
                        'positive_aspects': [],
                        'negative_aspects': [],
                        'key_issues': []
                    },
                    'raw_response': evaluation_text
                }
            
        except Exception as e:
            if attempt == max_retries - 1:
                return {
                    'success': False,
                    'error': str(e)
                }
            time.sleep(3)
    
    return {'success': False, 'error': 'Max retries exceeded'}


def extract_json_from_response(text, debug=False):
    """ä»å“åº”ä¸­æå–JSONï¼ˆç»ˆæå¢å¼ºç‰ˆï¼‰"""
    if not isinstance(text, str):
        return None
    text = text.strip()
    try:
        parsed = json.loads(text)
        if isinstance(parsed, dict) and 'rating' in parsed:
            if debug: print("[âœ… æ–¹æ³•1] ç›´æ¥è§£ææˆåŠŸ")
            return parsed
    except json.JSONDecodeError as e:
        if debug: print(f"[âŒ æ–¹æ³•1] ç›´æ¥è§£æå¤±è´¥: {e}")

    json_match = re.search(r'```json\s*(.*?)\s*```', text, re.DOTALL)
    if json_match:
        try:
            parsed = json.loads(json_match.group(1).strip())
            if isinstance(parsed, dict) and 'rating' in parsed:
                if debug: print("[âœ… æ–¹æ³•2] ä» ```json å—ä¸­æå–æˆåŠŸ")
                return parsed
        except json.JSONDecodeError as e:
            if debug: print(f"[âŒ æ–¹æ³•2] è§£æå¤±è´¥: {e}")

    code_match = re.search(r'```\s*(.*?)\s*```', text, re.DOTALL)
    if code_match:
        try:
            parsed = json.loads(code_match.group(1).strip())
            if isinstance(parsed, dict) and 'rating' in parsed:
                if debug: print("[âœ… æ–¹æ³•3] ä»æ™®é€šä»£ç å—ä¸­æå–æˆåŠŸ")
                return parsed
        except json.JSONDecodeError as e:
            if debug: print(f"[âŒ æ–¹æ³•3] è§£æå¤±è´¥: {e}")

    brace_match = re.search(r'\{.*\}', text, re.DOTALL)
    if brace_match:
        candidate = brace_match.group(0)
        try:
            parsed = json.loads(candidate)
            if isinstance(parsed, dict) and 'rating' in parsed:
                if debug: print("[âœ… æ–¹æ³•4] ä»å¤§æ‹¬å·å¯¹ä¸­æå–æˆåŠŸ")
                return parsed
        except json.JSONDecodeError as e:
            if debug: print(f"[âŒ æ–¹æ³•4] è§£æå¤±è´¥: {e}")

    rating_match = re.search(r'"rating"\s*:\s*"([ABCD])"', text, re.IGNORECASE)
    if rating_match:
        rating = rating_match.group(1).upper()
        reasoning_match = re.search(r'"reasoning"\s*:\s*"((?:[^"\\]|\\.)*)"', text, re.DOTALL)
        reasoning = reasoning_match.group(1) if reasoning_match else "æœªæå–åˆ°è¯¦ç»†ç†ç”±"
        if debug: print(f"[ğŸ› ï¸  æ–¹æ³•5] æ‰‹åŠ¨æ‹¼è£…æœ€å°JSONç»“æ„ (rating={rating})")
        return {"rating": rating, "reasoning": reasoning, "positive_aspects": [], "negative_aspects": [], "key_issues": []}

    if debug: print(f"[ğŸ’€ å¤±è´¥] æ‰€æœ‰æ–¹æ³•å‡å¤±è´¥ï¼Œå“åº”å‰100å­—ç¬¦: {repr(text[:100])}")
    return None


def extract_rating(text):
    """ä»æ–‡æœ¬ä¸­æå–è¯„çº§ï¼ˆæœ€åå…œåº•ï¼‰"""
    patterns = [r'"rating"\s*:\s*"([ABCD])"', r'rating.*?([ABCD])', r'[è¯„çº§è¯„åˆ†][:ï¼š\s]*([ABCD])', r'([ABCD])çº§', r'è¯„çº§\s*[ä¸ºæ˜¯]\s*([ABCD])']
    for pattern in patterns:
        match = re.search(pattern, text, re.IGNORECASE)
        if match and match.group(1).upper() in ['A', 'B', 'C', 'D']:
            return match.group(1).upper()
    for char in ['A', 'B', 'C', 'D']:
        if f'"{char}"' in text or f': {char}' in text or f' {char} ' in text:
            return char
    return 'Unknown'


def save_evaluation_results(task, model, evaluation_results):
    """ä¿å­˜è¯„ä¼°ç»“æœ"""
    output_dir = Path('evaluation_results') / task
    output_dir.mkdir(parents=True, exist_ok=True)
    safe_model_name = model.replace('/', '_').replace('\\', '_')
    output_file = output_dir / f"evaluation_{safe_model_name}.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(evaluation_results, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ è¯„ä¼°ç»“æœå·²ä¿å­˜: {output_file}")


def generate_evaluation_summary(task, all_evaluation_results):
    """ç”Ÿæˆè¯„ä¼°æ±‡æ€»ç»Ÿè®¡"""
    summary = {'task': task, 'evaluation_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'), 'models': {}, 'overall_statistics': {'A': 0, 'B': 0, 'C': 0, 'D': 0, 'Unknown': 0}}
    for model, results in all_evaluation_results.items():
        model_stats = {'total_scenarios': len(results), 'successful_evaluations': 0, 'failed_evaluations': 0, 'rating_distribution': {'A': 0, 'B': 0, 'C': 0, 'D': 0, 'Unknown': 0}, 'scenarios': []}
        for result in results:
            if result['evaluation_success']:
                model_stats['successful_evaluations'] += 1
                rating = result['evaluation']['rating']
                model_stats['rating_distribution'][rating] = model_stats['rating_distribution'].get(rating, 0) + 1
                summary['overall_statistics'][rating] += 1
                model_stats['scenarios'].append({'scenario': result.get('matched_scenario_key', result['scenario_class']), 'rating': rating, 'reasoning': result['evaluation'].get('reasoning', '')[:200]})
            else:
                model_stats['failed_evaluations'] += 1
        rating_scores = {'A': 4, 'B': 3, 'C': 2, 'D': 1, 'Unknown': 0}
        total_score = sum(model_stats['rating_distribution'][r] * rating_scores[r] for r in ['A', 'B', 'C', 'D'])
        total_rated = model_stats['successful_evaluations']
        model_stats['average_score'] = total_score / total_rated if total_rated > 0 else 0
        summary['models'][model] = model_stats
    output_dir = Path('evaluation_results') / task
    summary_file = output_dir / 'evaluation_summary.json'
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    print(f"ğŸ“Š æ±‡æ€»ç»Ÿè®¡å·²ä¿å­˜: {summary_file}")
    return summary


def print_evaluation_summary(summary):
    """æ‰“å°è¯„ä¼°æ±‡æ€»"""
    print("\n" + "="*80)
    print("è¯„ä¼°ç»“æœæ±‡æ€»")
    print("="*80 + "\n")
    sorted_models = sorted(summary['models'].items(), key=lambda x: x[1]['average_score'], reverse=True)
    print(f"{'æ’å':<5} {'æ¨¡å‹':<30} {'å¹³å‡åˆ†':<10} {'A':<5} {'B':<5} {'C':<5} {'D':<5} {'æˆåŠŸç‡':<10}")
    print("-" * 80)
    for rank, (model, stats) in enumerate(sorted_models, 1):
        dist = stats['rating_distribution']
        avg_score = stats['average_score']
        success_rate = stats['successful_evaluations'] / stats['total_scenarios'] * 100 if stats['total_scenarios'] > 0 else 0
        print(f"{rank:<5} {model:<30} {avg_score:<10.2f} {dist.get('A', 0):<5} {dist.get('B', 0):<5} {dist.get('C', 0):<5} {dist.get('D', 0):<5} {success_rate:<10.1f}%")
    print("\n" + "="*80 + "\n")


## ä¸»ç¨‹åº
if __name__ == "__main__":
    print("\n" + "="*80)
    print("LLMå›ç­”è´¨é‡è¯„ä¼°ç¨‹åºï¼ˆæ–­ç‚¹ç»­è¯„ç‰ˆï¼‰")
    print("="*80 + "\n")
    
    task = 'TASK1'
    evaluator_model = 'claude-sonnet-4-5-20250929'
    DEBUG_MODE = False
    
    print(f"ğŸ“‹ è¯„ä¼°ä»»åŠ¡: {task}")
    print(f"ğŸ” è¯„ä¼°æ¨¡å‹: {evaluator_model}")
    print(f"ğŸ› è°ƒè¯•æ¨¡å¼: {'å¼€å¯' if DEBUG_MODE else 'å…³é—­'}")
    print()
    
    load_dotenv()
    API_KEY = os.getenv("API_KEY_UiUi")
    base_url = "https://sg.uiuiapi.com/v1"
    
    if not API_KEY: raise ValueError("âŒ API_KEY_UiUi æœªåœ¨ç¯å¢ƒå˜é‡ä¸­æ‰¾åˆ°")
    
    print("ğŸ“„ åŠ è½½é…ç½®æ–‡ä»¶...")
    eval_config = load_evaluation_config(task)
    scenario_metadata_dict = load_scenario_metadata(task)
    eval_template = eval_config['evaluation_prompt_template']
    print(f"âœ… å·²åŠ è½½ {len([k for k in scenario_metadata_dict if k.startswith('åœºæ™¯')])} ä¸ªåœºæ™¯çš„å…ƒæ•°æ®\n")
    
    result_dir = Path('results') / task
    if not result_dir.exists(): exit(f"âŒ ç»“æœç›®å½•ä¸å­˜åœ¨: {result_dir}")
    
    models_to_evaluate = [f.stem.replace(f'{task.lower()}_', '') for f in result_dir.glob(f'{task.lower()}_*.json') if '_FAILED' not in f.name]
    if not models_to_evaluate: exit("âŒ æœªæ‰¾åˆ°ç»“æœæ–‡ä»¶")
    
    print(f"ğŸ¤– å¾…è¯„ä¼°æ¨¡å‹æ•°é‡: {len(models_to_evaluate)}")
    for i, m in enumerate(models_to_evaluate, 1): print(f"   {i}. {m}")
    print()
    
    print("="*80)
    print("å¼€å§‹è¯„ä¼°")
    print("="*80 + "\n")
    
    all_evaluation_results = {}
    
    for model_idx, model in enumerate(models_to_evaluate, 1):
        print(f"\n{'='*80}")
        print(f"[{model_idx}/{len(models_to_evaluate)}] è¯„ä¼°æ¨¡å‹: {model}")
        print(f"{'='*80}\n")
        
        model_results = load_model_results(task, model)
        if not model_results: continue
        
        print(f"ğŸ“ æ‰¾åˆ° {len(model_results)} æ¡å›ç­”è®°å½•\n")
        
        # â­ æ–­ç‚¹ç»­è¯„åŠŸèƒ½ï¼šåŠ è½½å·²æœ‰çš„æˆåŠŸè¯„ä¼°ç»“æœ
        existing_evaluations = {}
        safe_model_name = model.replace('/', '_').replace('\\', '_')
        eval_output_file = Path('evaluation_results') / task / f"evaluation_{safe_model_name}.json"
        
        if eval_output_file.exists():
            try:
                with open(eval_output_file, 'r', encoding='utf-8') as f:
                    previous_results = json.load(f)
                for prev_eval in previous_results:
                    if prev_eval.get('evaluation_success'):
                        existing_evaluations[prev_eval['original_question']] = prev_eval
                if existing_evaluations:
                    print(f"ğŸ”„ å·²åŠ è½½ {len(existing_evaluations)} æ¡æˆåŠŸçš„å†å²è¯„ä¼°è®°å½•ã€‚\n")
            except (json.JSONDecodeError, IOError):
                print(f"âš ï¸  æ— æ³•è§£æå†å²è¯„ä¼°æ–‡ä»¶: {eval_output_file}ï¼Œå°†é‡æ–°è¯„ä¼°æ‰€æœ‰åœºæ™¯ã€‚\n")

        evaluation_results = []
        success_count = 0
        fail_count = 0
        
        if DEBUG_MODE:
            print("âš ï¸  è°ƒè¯•æ¨¡å¼ï¼šä»…è¯„ä¼°å‰ 3 æ¡è®°å½•\n")
            model_results = model_results[:3]
        
        for idx, result in enumerate(model_results, 1):
            scenario_class = result['scenario_class']
            user_question = result['prompt_role_user']
            ai_response = result['response']
            
            display_text = user_question[:60] + "..." if len(user_question) > 60 else user_question
            print(f"[{idx}/{len(model_results)}] {display_text:65s}", end='', flush=True)

            # â­ æ–­ç‚¹ç»­è¯„åŠŸèƒ½ï¼šæ£€æŸ¥æ˜¯å¦å¯ä»¥è·³è¿‡
            if user_question in existing_evaluations:
                print(" âœ… å·²è¯„ä¼°ï¼Œè·³è¿‡")
                evaluation_results.append(existing_evaluations[user_question])
                success_count += 1
                continue

            try:
                matched_key, scenario_metadata = find_matching_scenario(scenario_class, user_question, scenario_metadata_dict)
                if not matched_key:
                    print(" âš ï¸  æœªåŒ¹é…")
                    fail_count += 1
                    evaluation_results.append({'scenario_class': scenario_class, 'matched_scenario_key': None, 'original_question': user_question, 'original_response': ai_response, 'evaluation_success': False, 'error': 'No matching scenario found'})
                    continue
                
                print(f" ğŸ“Œ {matched_key:12s}", end='', flush=True)
                
                evaluation_prompt = build_evaluation_prompt(eval_template, scenario_metadata, user_question, ai_response, matched_key)
                eval_result = evaluate_response(API_KEY, base_url, evaluator_model, evaluation_prompt, debug=DEBUG_MODE)
                
                if eval_result['success']:
                    rating = eval_result['evaluation']['rating']
                    print(f" âœ… {rating}")
                    success_count += 1
                else:
                    error_msg = eval_result.get('error', 'æœªçŸ¥é”™è¯¯').replace('\n', ' ')[:30]
                    print(f" âŒ {error_msg}")
                    fail_count += 1
                
                evaluation_results.append({'scenario_class': scenario_class, 'matched_scenario_key': matched_key, 'original_question': user_question, 'original_response': ai_response, 'evaluation_success': eval_result['success'], 'evaluation': eval_result.get('evaluation', {}), 'evaluation_raw': eval_result.get('raw_response', ''), 'error': eval_result.get('error', '')})
                
            except Exception as e:
                print(f" âŒ è„šæœ¬å¼‚å¸¸: {str(e)[:30]}")
                fail_count += 1
                evaluation_results.append({'scenario_class': scenario_class, 'matched_scenario_key': None, 'original_question': user_question, 'original_response': ai_response, 'evaluation_success': False, 'error': str(e)})
            
            if not DEBUG_MODE:
                time.sleep(2)
        
        save_evaluation_results(task, model, evaluation_results)
        print(f"\nâœ… æ¨¡å‹ {model} è¯„ä¼°å®Œæˆ:")
        print(f"   - æˆåŠŸ: {success_count}")
        print(f"   - å¤±è´¥: {fail_count}")
        if success_count + fail_count > 0: print(f"   - æˆåŠŸç‡: {success_count/(success_count+fail_count)*100:.1f}%")
        
        all_evaluation_results[model] = evaluation_results
        
        if DEBUG_MODE:
            print("\nâš ï¸  è°ƒè¯•æ¨¡å¼ï¼šè·³è¿‡å‰©ä½™æ¨¡å‹")
            break
    
    print(f"\n{'='*80}")
    print("ç”Ÿæˆæ±‡æ€»ç»Ÿè®¡")
    print(f"{'='*80}\n")
    
    summary = generate_evaluation_summary(task, all_evaluation_results)
    print_evaluation_summary(summary)
    
    print(f"\n{'='*80}")
    print("ğŸ‰ è¯„ä¼°å®Œæˆï¼")
    print(f"{'='*80}\n")