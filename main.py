import os
from openai import OpenAI
from dotenv import load_dotenv
import time
from typing import List, Dict
import yaml
from pathlib import Path
import json
from datetime import datetime
import pypandoc
from tqdm import tqdmÃŸ


## å‡½æ•°å°è£…

def LLM_request(api_key, base_url, model, scenario_class, messages, max_retries=2):
    """è®¿é—®LLMï¼ˆå¢å¼ºé”™è¯¯å¤„ç†+é‡è¯•+è¿›åº¦æ˜¾ç¤ºï¼‰"""
    
    for attempt in range(max_retries):
        try:
            if attempt == 0:
                print(f"   ğŸ“¤ {scenario_class[:40]:40s}", end='', flush=True)
            else:
                print(f"   ğŸ”„ é‡è¯•ç¬¬{attempt}æ¬¡...", end='', flush=True)
            
            start_time = time.time()
            client = OpenAI(api_key=api_key, base_url=base_url)
            
            # ç¬¬ä¸€æ¬¡120ç§’ï¼Œé‡è¯•æ—¶180ç§’
            timeout_value = 120 if attempt == 0 else 180
            
            response = client.chat.completions.create(
                model=model, 
                messages=messages, 
                stream=False,
                timeout=timeout_value
            )
            
            elapsed = time.time() - start_time
            print(f" âœ… {elapsed:.1f}s")

            return {
                'model': model,
                'scenario_class': scenario_class,
                'prompt': messages,
                'response': response.choices[0].message.content,
                'success': True,
                'elapsed_time': elapsed
            }
            
        except Exception as e:
            elapsed = time.time() - start_time
            error_type = type(e).__name__
            error_msg = str(e)
            
            # åˆ¤æ–­æ˜¯å¦ä¸ºè¶…æ—¶é”™è¯¯
            is_timeout = 'timeout' in error_msg.lower() or error_type == 'Timeout'
            is_last_attempt = (attempt == max_retries - 1)
            
            if is_timeout and not is_last_attempt:
                print(f" â±ï¸ è¶…æ—¶({elapsed:.0f}s)")
                continue
            
            # æœ€åä¸€æ¬¡å¤±è´¥
            print(f" âŒ {error_type}")
            print(f"      é”™è¯¯: {error_msg[:100]}")
            
            return {
                'model': model,
                'scenario_class': scenario_class,
                'prompt': messages,
                'response': '',
                'success': False,
                'error': error_msg,
                'elapsed_time': elapsed
            }
    
    return {'success': False, 'error': 'Max retries exceeded'}


def LLM_results_save(task:str, model:str, LLM_answers):
    """ä¿å­˜LLMè¾“å‡ºç»“æœåˆ°.jsonæ–‡ä»¶ï¼ˆåªä¿å­˜æˆåŠŸçš„ç»“æœï¼‰"""
    results = []
    
    for answer in LLM_answers:
        record = {
            'scenario_class': answer.get('scenario_class',''),
            'prompt_role_system': answer.get('prompt',[])[0].get('content','') if answer.get('prompt') else '',
            'prompt_role_user': answer.get('prompt',[])[1].get('content','') if answer.get('prompt') else '',
            'response': answer.get('response',''),
            'elapsed_time': answer.get('elapsed_time', 0),
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
        results.append(record)

    # ç¡®ä¿ç›®å½•å­˜åœ¨
    result_file_path = f'results/{task}'
    os.makedirs(result_file_path, exist_ok=True)
    
    # å¤„ç†æ¨¡å‹åä¸­å¯èƒ½çš„ç‰¹æ®Šå­—ç¬¦
    safe_model_name = model.replace('/', '_').replace('\\', '_')
    result_file_name = f"{task.lower()}_{safe_model_name}.json"
    full_path = os.path.join(result_file_path, result_file_name)
    
    with open(full_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜: {full_path} ({len(results)} æ¡æœ‰æ•ˆè®°å½•)")


def save_failed_scenarios(task:str, model:str, failed_list):
    """ä¿å­˜å¤±è´¥çš„åœºæ™¯è®°å½•"""
    if not failed_list:
        return
    
    result_file_path = f'results/{task}'
    os.makedirs(result_file_path, exist_ok=True)
    
    safe_model_name = model.replace('/', '_').replace('\\', '_')
    failed_file_name = f"{task.lower()}_{safe_model_name}_FAILED.json"
    full_path = os.path.join(result_file_path, failed_file_name)
    
    with open(full_path, 'w', encoding='utf-8') as f:
        json.dump(failed_list, f, ensure_ascii=False, indent=2)
    
    print(f"âš ï¸  å¤±è´¥è®°å½•å·²ä¿å­˜: {full_path} ({len(failed_list)} æ¡)")


def markdown_to_word(markdown_text, output_path):
    """Markdownè½¬Word"""
    try:
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        output = pypandoc.convert_text(
            markdown_text, 
            'docx', 
            format='md', 
            outputfile=output_path
        )
        print(f"âœ“ Wordæ–‡æ¡£å·²ç”Ÿæˆ: {output_path}")
        return True
    except Exception as e:
        print(f"âœ— è½¬æ¢å¤±è´¥: {e}")
        return False


def get_combined_markdown_content(data):
    """åˆ›å»ºåŒ…å«æ‰€æœ‰åœºæ™¯çš„åˆå¹¶Markdownå†…å®¹"""
    
    combined_content = f"""# AIå¯¹è¯è®°å½•æ±‡æ€»æŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**æ€»è®°å½•æ•°**: {len(data)}

---
"""
    
    for i, item in enumerate(data, 1):
        scenario_class = item.get('scenario_class', f'åœºæ™¯_{i}')
        system_prompt = item.get('prompt_role_system', '')
        user_prompt = item.get('prompt_role_user', '')
        response_content = item.get('response', '')
        timestamp = item.get('timestamp', '')
        elapsed_time = item.get('elapsed_time', 0)
        
        combined_content += f"""
## {i}. {scenario_class}

### åŸºæœ¬ä¿¡æ¯
- **ç”Ÿæˆæ—¶é—´**: {timestamp}
- **å“åº”æ—¶é—´**: {elapsed_time:.1f}ç§’
- **åœºæ™¯åˆ†ç±»**: {scenario_class}
- **è®°å½•ç¼–å·**: ç¬¬{i}æ¡

### å¯¹è¯è®¾ç½®
**ç³»ç»Ÿè§’è‰²è®¾å®š**:
{system_prompt}

**ç”¨æˆ·é—®é¢˜**:
{user_prompt}

### AIå›å¤å†…å®¹
{response_content}

---
"""
    
    combined_content += f"\n\n*æ–‡æ¡£ç”Ÿæˆå®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*"
    return combined_content


def convert_json_to_single_word(json_file_path):
    """å°†JSONæ–‡ä»¶çš„æ‰€æœ‰å†…å®¹åˆå¹¶åˆ°ä¸€ä¸ªWordæ–‡ä»¶ä¸­"""
    
    with open(json_file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print(f"ğŸ“ æ‰¾åˆ° {len(data)} æ¡è®°å½•")
    
    json_dir = os.path.dirname(json_file_path)
    json_filename = os.path.basename(json_file_path)
    word_filename = os.path.splitext(json_filename)[0] + '.docx'
    output_path = os.path.join(json_dir, word_filename)
    
    combined_markdown = get_combined_markdown_content(data)
    
    if markdown_to_word(combined_markdown, output_path):
        print(f"âœ… æˆåŠŸç”Ÿæˆåˆå¹¶Wordæ–‡ä»¶: {output_path}")
        return True
    else:
        print(f"âŒ ç”ŸæˆWordæ–‡ä»¶å¤±è´¥")
        return False


def convert_all_json_to_word(task):
    """å°†æŸä¸ªä»»åŠ¡ä¸‹çš„æ‰€æœ‰JSONæ–‡ä»¶è½¬æ¢ä¸ºWordï¼ˆæ’é™¤FAILEDæ–‡ä»¶ï¼‰"""
    result_path = f'results/{task}'
    
    if not os.path.exists(result_path):
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {result_path}")
        return
    
    # åªè½¬æ¢éFAILEDçš„JSONæ–‡ä»¶
    json_files = [f for f in Path(result_path).glob('*.json') if '_FAILED' not in f.name]
    
    if not json_files:
        print(f"âŒ æœªæ‰¾åˆ°JSONæ–‡ä»¶: {result_path}")
        return
    
    print(f"ğŸ“ æ‰¾åˆ° {len(json_files)} ä¸ªJSONæ–‡ä»¶")
    
    success_count = 0
    fail_count = 0
    
    for json_file in tqdm(json_files, desc="è½¬æ¢è¿›åº¦", ncols=80):
        try:
            if convert_json_to_single_word(str(json_file)):
                success_count += 1
            else:
                fail_count += 1
        except Exception as e:
            print(f"âŒ è½¬æ¢å¤±è´¥: {json_file.name} - {e}")
            fail_count += 1
    
    print(f"\nè½¬æ¢å®Œæˆ: æˆåŠŸ {success_count}, å¤±è´¥ {fail_count}")


## ç¨‹åºå…¥å£
if __name__ == "__main__":
    
    print("\n" + "="*60)
    print("LLMå¯¹æ¯”æµ‹è¯•ç¨‹åº")
    print("="*60 + "\n")

    ## ä»»åŠ¡æç¤ºè¯é€‰æ‹©
    '''
    ä»»åŠ¡åç§°ï¼š
    TASK_1: äº¤äº’é²æ£’æ€§è¯„ä¼°
    TASK_2: xxxxxxxxxè¯„ä¼°
    TASK_3: xxxxxxxxxè¯„ä¼°
    TASK_4: xxxxxxxxxè¯„ä¼°
    TASK_5: xxxxxxxxxè¯„ä¼°
    '''
    target_task = 'TASK1'
    print(f"ğŸ“‹ ä»»åŠ¡: {target_task}")


    ## æ¨¡å‹åˆ—è¡¨
    model_list = [
        'gpt-5', 'gpt-4o', 'gemini-2.5-flash', 'grok-3', 
        'deepseek-chat', 'qwen-plus', 'kimi-k2-thinking', 
        'doubao-pro-256k', 'glm-4.6'
    ]
    
    print(f"ğŸ¤– æµ‹è¯•æ¨¡å‹æ•°é‡: {len(model_list)}")
    for i, m in enumerate(model_list, 1):
        print(f"   {i}. {m}")
    print()


    ## APIé…ç½®
    load_dotenv()
    API_KEY_UiUi = os.getenv("API_KEY_UiUi")
    if not API_KEY_UiUi:
        raise ValueError("âŒ API_KEY_UiUi æœªåœ¨ç¯å¢ƒå˜é‡ä¸­æ‰¾åˆ°")


    ## ç«™ç‚¹ç½‘å€
    base_url = "https://sg.uiuiapi.com/v1"


    ## find ä»»åŠ¡æç¤ºè¯é…ç½®æ–‡ä»¶
    current_file_path = Path(__file__).resolve()
    project_root_path = current_file_path.parent
    task_prompt_filename = f'{target_task}_prompts.yaml'
    config_file_path = project_root_path / 'config' / task_prompt_filename
    
    print(f"ğŸ“„ é…ç½®æ–‡ä»¶: {config_file_path}")
    
    if not os.path.exists(config_file_path):
        raise FileNotFoundError(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file_path}")

    with open(config_file_path, 'r', encoding='utf-8') as f:
        task_config = yaml.safe_load(f) or {}

    if not isinstance(task_config, dict):
        raise ValueError("âŒ é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼Œåº”ä¸ºå­—å…¸ç»“æ„")
    
    print(f"ğŸ“ åœºæ™¯æ•°é‡: {len(task_config)}")
    print()


    ## å¼€å§‹æµ‹è¯•
    print("="*60)
    print("å¼€å§‹æ‰§è¡Œæµ‹è¯•")
    print("="*60 + "\n")
    
    overall_start_time = time.time()
    
    # ç»Ÿè®¡æ‰€æœ‰æ¨¡å‹çš„æ€»ä½“æƒ…å†µ
    overall_stats = {
        'total_success': 0,
        'total_fail': 0,
        'model_stats': {}
    }


    # å¤šå±‚éå†ï¼šä¾æ¬¡éå†æ¯ä¸ªæ¨¡å‹
    for model_idx, model in enumerate(model_list, 1):
        print(f"\n{'='*60}")
        print(f"[{model_idx}/{len(model_list)}] æµ‹è¯•æ¨¡å‹: {model}")
        print(f"{'='*60}\n")
        
        model_start_time = time.time()
        LLM_answers = []
        failed_scenarios = []  # è®°å½•å¤±è´¥çš„åœºæ™¯
        success_count = 0
        fail_count = 0
        
        scenarios = list(task_config.items())
        
        for scenario_class, scenario_config in tqdm(scenarios, desc=f"è¿›åº¦", ncols=80):
            role_system_content = scenario_config['role_system_content']
            role_user_content = scenario_config['role_user_content']
            
            for prompt in role_user_content:
                messages = [
                    {"role": "system", "content": role_system_content}, 
                    {"role": "user", "content": prompt}
                ]

                # è°ƒç”¨LLM
                LLM_answer = LLM_request(
                    api_key=API_KEY_UiUi, 
                    base_url=base_url, 
                    model=model, 
                    scenario_class=scenario_class, 
                    messages=messages,
                    max_retries=2
                )

                # åªä¿å­˜æˆåŠŸçš„ç»“æœ
                if LLM_answer.get('success', False):
                    LLM_answers.append(LLM_answer)
                    success_count += 1
                else:
                    fail_count += 1
                    # è®°å½•å¤±è´¥çš„åœºæ™¯ä¿¡æ¯
                    failed_scenarios.append({
                        'scenario_class': scenario_class,
                        'role_system_content': role_system_content,
                        'role_user_content': prompt,
                        'error': LLM_answer.get('error', 'Unknown error'),
                        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    })

                time.sleep(2)

        # ä¿å­˜ç»“æœ
        model_time = time.time() - model_start_time
        
        if LLM_answers:
            LLM_results_save(task=target_task, model=model, LLM_answers=LLM_answers)
            
            # è®¡ç®—å¹³å‡å“åº”æ—¶é—´
            avg_time = sum(a.get('elapsed_time', 0) for a in LLM_answers) / len(LLM_answers)
            
            print(f"\nâœ… æ¨¡å‹ {model} å®Œæˆ:")
            print(f"   - æˆåŠŸ: {success_count}")
            print(f"   - å¤±è´¥: {fail_count}")
            print(f"   - æˆåŠŸç‡: {success_count/(success_count+fail_count)*100:.1f}%")
            print(f"   - æ€»è€—æ—¶: {model_time:.1f}ç§’ ({model_time/60:.1f}åˆ†é’Ÿ)")
            print(f"   - å¹³å‡å“åº”: {avg_time:.1f}ç§’")
            
            # è®°å½•ç»Ÿè®¡ä¿¡æ¯
            overall_stats['model_stats'][model] = {
                'success': success_count,
                'fail': fail_count,
                'success_rate': success_count/(success_count+fail_count)*100,
                'avg_time': avg_time
            }
        else:
            print(f"\nâš ï¸  æ¨¡å‹ {model} å…¨éƒ¨å¤±è´¥")
            print(f"   - å¤±è´¥: {fail_count}")
            
            overall_stats['model_stats'][model] = {
                'success': 0,
                'fail': fail_count,
                'success_rate': 0,
                'avg_time': 0
            }
        
        # ä¿å­˜å¤±è´¥è®°å½•
        if failed_scenarios:
            save_failed_scenarios(task=target_task, model=model, failed_list=failed_scenarios)
        
        # æ›´æ–°æ€»ä½“ç»Ÿè®¡
        overall_stats['total_success'] += success_count
        overall_stats['total_fail'] += fail_count


    # æ€»ç»“
    overall_time = time.time() - overall_start_time
    print(f"\n{'='*60}")
    print(f"âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
    print(f"{'='*60}\n")
    print(f"æ€»ä½“ç»Ÿè®¡:")
    print(f"   - æ€»æˆåŠŸ: {overall_stats['total_success']}")
    print(f"   - æ€»å¤±è´¥: {overall_stats['total_fail']}")
    print(f"   - æ€»æˆåŠŸç‡: {overall_stats['total_success']/(overall_stats['total_success']+overall_stats['total_fail'])*100:.1f}%")
    print(f"   - æ€»è€—æ—¶: {overall_time:.1f}ç§’ ({overall_time/60:.1f}åˆ†é’Ÿ)")
    print()
    
    # æ˜¾ç¤ºå„æ¨¡å‹æˆåŠŸç‡æ’å
    print(f"å„æ¨¡å‹æˆåŠŸç‡æ’å:")
    sorted_models = sorted(overall_stats['model_stats'].items(), 
                          key=lambda x: x[1]['success_rate'], 
                          reverse=True)
    for rank, (model, stats) in enumerate(sorted_models, 1):
        print(f"   {rank}. {model:25s} - {stats['success_rate']:5.1f}% ({stats['success']}/{stats['success']+stats['fail']})")
    
    print(f"\n{'='*60}\n")


    ## è‡ªåŠ¨è½¬æ¢æ‰€æœ‰JSONåˆ°Word
    print(f"\n{'='*60}")
    print(f"å¼€å§‹è½¬æ¢ç»“æœæ–‡ä»¶")
    print(f"{'='*60}\n")
    
    convert_all_json_to_word(target_task)
    
    
    ## ç”Ÿæˆå¤±è´¥åœºæ™¯æ±‡æ€»æŠ¥å‘Šï¼ˆå¯é€‰ï¼‰
    result_path = f'results/{target_task}'
    failed_files = list(Path(result_path).glob('*_FAILED.json'))
    
    if failed_files:
        print(f"\n{'='*60}")
        print(f"å¤±è´¥åœºæ™¯æ±‡æ€»")
        print(f"{'='*60}\n")
        print(f"âš ï¸  å…±æœ‰ {len(failed_files)} ä¸ªæ¨¡å‹å­˜åœ¨å¤±è´¥åœºæ™¯:")
        
        for failed_file in failed_files:
            with open(failed_file, 'r', encoding='utf-8') as f:
                failed_data = json.load(f)
            
            model_name = failed_file.stem.replace(f'{target_task.lower()}_', '').replace('_FAILED', '')
            print(f"   - {model_name:25s}: {len(failed_data)} ä¸ªå¤±è´¥åœºæ™¯")
        
        print(f"\næç¤º: ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤é‡è·‘å¤±è´¥åœºæ™¯:")
        print(f"   python retry_failed.py {target_task} <æ¨¡å‹å>")
        print(f"   æˆ–è¿è¡Œ: ./retry_all_failed.sh (æ‰¹é‡é‡è·‘)")
    else:
        print(f"\nğŸ‰ æ‰€æœ‰åœºæ™¯å…¨éƒ¨æˆåŠŸï¼Œæ— å¤±è´¥è®°å½•ï¼")
    
    
    print(f"\n{'='*60}")
    print(f"ğŸ‰ å…¨éƒ¨æµç¨‹å®Œæˆï¼")
    print(f"{'='*60}\n")