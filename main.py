import os
from openai import OpenAI
from dotenv import load_dotenv
import time
from typing import List, Dict
import yaml
from pathlib import Path
import json
from datetime import datetime
import pypandoc



## å‡½æ•°å°è£…
# è®¿é—®LLM
def LLM_request(api_key, base_url, model, scenario_class, messages):
    try:
        client = OpenAI(api_key=api_key, base_url=base_url)

        # response = client.chat.completions.create(model=model, messages=messages, stream=False, timeout=30)
        response = client.chat.completions.create(model=model, messages=messages, stream=False)

        return {
            'model': model,
            'scenario_class': scenario_class,
            'prompt': messages,
            'response': response.choices[0].message.content,
            'success': True
        }
    except Exception as e:
        print(f"LLMè¯·æ±‚å¤±è´¥: {model} - {messages} - é”™è¯¯: {str(e)}")
        return {
            'success': False
        }

# ä¿å­˜LLMè¾“å‡ºç»“æœåˆ°.jsonæ–‡ä»¶
def LLM_results_save(task:str, model:str, LLM_answers):
    results = []
    for answer in LLM_answers:
        record = {
            'scenario_class': answer.get('scenario_class','') if answer.get('scenario_class') else '',
            'prompt_role_system': answer.get('prompt',[])[0].get('content','') if answer.get('prompt') else '',
            'prompt_role_user': answer.get('prompt',[])[1].get('content','') if answer.get('prompt') else '',
            'response': answer.get('response','') if answer.get('response') else '',
            'timestamp': datetime.now().strftime('%Y-%m-%d-%H:%M')
        }
        results.append(record)

    result_file_name = task.lower() + '_' + model + '.json'
    result_file_path = 'results/TASK1'
    with open(os.path.join(result_file_path, result_file_name), 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)


def markdown_to_word(markdown_text, output_path):
    """Markdownè½¬Word"""
    try:
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        output = pypandoc.convert_text(
            markdown_text, 
            'docx', 
            format='md', 
            outputfile=output_path
        )
        print(f"âœ“ Wordæ–‡æ¡£å·²ç”Ÿæˆ: {output_path}")
        return True
    except Exception as e:
        print(f"âœ— è½¬æ¢å¤±è´¥: {e}")
        return False


def get_combined_markdown_content(data):
    """åˆ›å»ºåŒ…å«æ‰€æœ‰åœºæ™¯çš„åˆå¹¶Markdownå†…å®¹"""
    
    combined_content = f"""# AIå¯¹è¯è®°å½•æ±‡æ€»æŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M')}
**æ€»è®°å½•æ•°**: {len(data)}

---
"""
    
    for i, item in enumerate(data, 1):
        scenario_class = item.get('scenario_class', f'åœºæ™¯_{i}')
        system_prompt = item.get('prompt_role_system', '')
        user_prompt = item.get('prompt_role_user', '')
        response_content = item.get('response', '')
        timestamp = item.get('timestamp', '')
        
        # ä¸ºæ¯ä¸ªåœºæ™¯æ·»åŠ åˆ†éš”ç¬¦å’Œæ ‡é¢˜
        combined_content += f"""
## {i}. {scenario_class}

### åŸºæœ¬ä¿¡æ¯
- **ç”Ÿæˆæ—¶é—´**: {timestamp}
- **åœºæ™¯åˆ†ç±»**: {scenario_class}
- **è®°å½•ç¼–å·**: ç¬¬{i}æ¡

### å¯¹è¯è®¾ç½®
**ç³»ç»Ÿè§’è‰²è®¾å®š**:
{system_prompt}

**ç”¨æˆ·é—®é¢˜**:
{user_prompt}

### AIå›å¤å†…å®¹
{response_content}

---
"""
    
    combined_content += f"\n\n*æ–‡æ¡£ç”Ÿæˆå®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M')}*"
    return combined_content

def convert_json_to_single_word(json_file_path):
    """å°†JSONæ–‡ä»¶çš„æ‰€æœ‰å†…å®¹åˆå¹¶åˆ°ä¸€ä¸ªWordæ–‡ä»¶ä¸­"""
    
    # è¯»å–JSONæ–‡ä»¶
    with open(json_file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print(f"ğŸ“ æ‰¾åˆ° {len(data)} æ¡è®°å½•")
    
    # ç”Ÿæˆä¸JSONæ–‡ä»¶å¹³è¡Œçš„Wordæ–‡ä»¶è·¯å¾„
    json_dir = os.path.dirname(json_file_path)
    json_filename = os.path.basename(json_file_path)
    word_filename = os.path.splitext(json_filename)[0] + '.docx'
    output_path = os.path.join(json_dir, word_filename)
    
    # åˆ›å»ºåˆå¹¶çš„Markdownå†…å®¹
    combined_markdown = get_combined_markdown_content(data)
    
    # è½¬æ¢ä¸ºWord
    if markdown_to_word(combined_markdown, output_path):
        print(f"âœ… æˆåŠŸç”Ÿæˆåˆå¹¶Wordæ–‡ä»¶: {output_path}")
        return True
    else:
        print(f"âŒ ç”ŸæˆWordæ–‡ä»¶å¤±è´¥")
        return False


## ç¨‹åºå…¥å£
if __name__ == "__main__":


    ## ä»»åŠ¡æç¤ºè¯é€‰æ‹©
    '''
    ä»»åŠ¡åç§°ï¼š
    TASK_1: äº¤äº’é²æ£’æ€§è¯„ä¼°
    TASK_2: xxxxxxxxxè¯„ä¼°
    TASK_3: xxxxxxxxxè¯„ä¼°
    TASK_4: xxxxxxxxxè¯„ä¼°
    TASK_5: xxxxxxxxxè¯„ä¼°
    '''
    target_task = 'TASK1'         # ?? é€‰æ‹©ä¸€ä¸ªä»»åŠ¡


    ## æ¨¡å‹åˆ—è¡¨
    # model_list = ['gpt-5', 'claude-3', 'gemini-2.5', 'grok-3', 'deepseek-r1', 'qwen-plus', 'kimi-k2', 'doubao-seed', 'glm-4.6-thinking']
    model_list = [ 'qwen-plus']   # ?? é€‰æ‹©ä¸€ä¸ªæˆ–å¤šä¸ªéœ€è¦æ‰§è¡Œçš„LLM


    ## APIé…ç½®
    load_dotenv()   # ä»ç¯å¢ƒå˜é‡ä¸­è·å–æ¨¡å‹APIå¯†é’¥
    API_KEY_UiUi = os.getenv("API_KEY_UiUi")
    if not API_KEY_UiUi:
        raise ValueError("API_KEY_UiUi æœªåœ¨ç¯å¢ƒå˜é‡ä¸­æ‰¾åˆ°")


    ## ç«™ç‚¹ç½‘å€
    base_url="https://sg.uiuiapi.com/v1"


    ## find ä»»åŠ¡æç¤ºè¯é…ç½®æ–‡ä»¶
    current_file_path = Path(__file__).resolve()
    project_root_path = current_file_path.parent
    task_prompt_filename = f'{target_task}_prompts.yaml'
    config_file_path = project_root_path / 'config' / task_prompt_filename
    print(config_file_path)
    if not os.path.exists(config_file_path):
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file_path}")

    with open(config_file_path, 'r', encoding='utf-8') as f:
        task_config = yaml.safe_load(f) or {}

    # éªŒè¯é…ç½®ç»“æ„
    if not isinstance(task_config, dict):
        raise ValueError("é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼Œåº”ä¸ºå­—å…¸ç»“æ„")


    ## å¤šå±‚éå†ï¼šä¾æ¬¡éå†æ¯ä¸ªprompt
    # for model in model_list:
    #     LLM_answers = []
    #     for scenario_class, scenario_config in task_config.items():
    #         # print(f'scenario_class: {scenario_class}')
    #         role_system_content = scenario_config['role_system_content']
    #         role_user_content = scenario_config['role_user_content']
    #         for prompt in role_user_content:
    #             messages=[
    #                 {"role": "system", "content": role_system_content}, 
    #                 {"role": "user", "content": prompt}
    #             ]
    #             # print(messages)

    #             # å°†messagesä¼ é€’ç»™å‰é¢çš„LLMè®¿é—®è¯·æ±‚å‡½æ•°
    #             LLM_answer = LLM_request(api_key=API_KEY_UiUi, base_url=base_url, model=model, scenario_class=scenario_class, messages=messages)

    #             # role_system_content = LLM_answer.get('prompt',[])[0].get('content','') if LLM_answer.get('prompt') else ''
    #             # print(f'role_system_content: {role_system_content}')
    #             # print(LLM_answer)

    #             LLM_answers.append(LLM_answer)

    #             time.sleep(2)

    #     LLM_results_save(task=target_task, model=model, LLM_answers=LLM_answers)


    ## LLMä¿å­˜ç»“æœåˆ°jsonæ–‡ä»¶åï¼Œè½¬æ¢ä¸ºwordæ–‡ä»¶
    convert_json_to_single_word("results/TASK1/task1_qwen-plus.json")
